{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "y = X**2 + np.random.normal(0, 0.1, size=X.shape)\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(X, y, alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=1))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_gan(generator, discriminator):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    z = Input(shape=(1,))\n",
    "    img = generator(z)\n",
    "    validity = discriminator(img)\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, combined, epochs, batch_size=128):\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "        real_imgs = X[idx]\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, 1))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        noise = np.random.normal(0, 1, (batch_size, 1))\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grupo</th>\n",
       "      <th>btsId</th>\n",
       "      <th>btsNetNome</th>\n",
       "      <th>lat_x</th>\n",
       "      <th>lon_x</th>\n",
       "      <th>cch</th>\n",
       "      <th>azimuth</th>\n",
       "      <th>ponto_id</th>\n",
       "      <th>delay_1</th>\n",
       "      <th>delay_2</th>\n",
       "      <th>delay_3</th>\n",
       "      <th>lat_y</th>\n",
       "      <th>lon_y</th>\n",
       "      <th>indoor</th>\n",
       "      <th>rssi_id</th>\n",
       "      <th>rssi_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BTS_1_1</td>\n",
       "      <td>PEMDL01_1</td>\n",
       "      <td>-8.04342</td>\n",
       "      <td>-34.952</td>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>6604</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-8.055786</td>\n",
       "      <td>-34.951505</td>\n",
       "      <td>True</td>\n",
       "      <td>rssi_1_1</td>\n",
       "      <td>0.261743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>BTS_1_1</td>\n",
       "      <td>PEMDL01_1</td>\n",
       "      <td>-8.04342</td>\n",
       "      <td>-34.952</td>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>5515</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-8.055784</td>\n",
       "      <td>-34.951608</td>\n",
       "      <td>True</td>\n",
       "      <td>rssi_1_1</td>\n",
       "      <td>0.228380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>BTS_1_1</td>\n",
       "      <td>PEMDL01_1</td>\n",
       "      <td>-8.04342</td>\n",
       "      <td>-34.952</td>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>5049</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-8.055506</td>\n",
       "      <td>-34.951693</td>\n",
       "      <td>True</td>\n",
       "      <td>rssi_1_1</td>\n",
       "      <td>0.285777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>BTS_1_1</td>\n",
       "      <td>PEMDL01_1</td>\n",
       "      <td>-8.04342</td>\n",
       "      <td>-34.952</td>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>8737</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-8.050390</td>\n",
       "      <td>-34.959630</td>\n",
       "      <td>False</td>\n",
       "      <td>rssi_1_1</td>\n",
       "      <td>0.581431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>BTS_1_1</td>\n",
       "      <td>PEMDL01_1</td>\n",
       "      <td>-8.04342</td>\n",
       "      <td>-34.952</td>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>1203</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-8.055380</td>\n",
       "      <td>-34.951736</td>\n",
       "      <td>True</td>\n",
       "      <td>rssi_1_1</td>\n",
       "      <td>0.143584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   grupo    btsId btsNetNome    lat_x   lon_x  cch  azimuth  ponto_id  \\\n",
       "0      1  BTS_1_1  PEMDL01_1 -8.04342 -34.952  873        0      6604   \n",
       "1      1  BTS_1_1  PEMDL01_1 -8.04342 -34.952  873        0      5515   \n",
       "2      1  BTS_1_1  PEMDL01_1 -8.04342 -34.952  873        0      5049   \n",
       "3      1  BTS_1_1  PEMDL01_1 -8.04342 -34.952  873        0      8737   \n",
       "4      1  BTS_1_1  PEMDL01_1 -8.04342 -34.952  873        0      1203   \n",
       "\n",
       "   delay_1  delay_2  delay_3     lat_y      lon_y  indoor   rssi_id  \\\n",
       "0      5.0      3.0      2.0 -8.055786 -34.951505    True  rssi_1_1   \n",
       "1      5.0      3.0      2.0 -8.055784 -34.951608    True  rssi_1_1   \n",
       "2      5.0      3.0      2.0 -8.055506 -34.951693    True  rssi_1_1   \n",
       "3      4.0      7.0      2.0 -8.050390 -34.959630   False  rssi_1_1   \n",
       "4      5.0      3.0      2.0 -8.055380 -34.951736    True  rssi_1_1   \n",
       "\n",
       "   rssi_value  \n",
       "0    0.261743  \n",
       "1    0.228380  \n",
       "2    0.285777  \n",
       "3    0.581431  \n",
       "4    0.143584  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/train_normalized_dataset.csv')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:719: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5990: RuntimeWarning: divide by zero encountered in power\n",
      "  return cd2*x**(c-1)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:719: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minpack_py.py:177: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:719: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minpack_py.py:177: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minpack_py.py:177: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:719: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:719: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minpack_py.py:177: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "c:\\Users\\iantr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5999: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('data/train_normalized_dataset.csv')\n",
    "\n",
    "data.drop(columns=['indoor'], inplace=True)\n",
    "data.drop(columns=['rssi_id'], inplace=True)\n",
    "data.drop(columns=['btsNetNome'], inplace=True)\n",
    "data.drop(columns=['btsId'], inplace=True)\n",
    "\n",
    "# Instantiate and fit the Gaussian Copula model\n",
    "copula = GaussianMultivariate()\n",
    "copula.fit(data)\n",
    "\n",
    "# Sample synthetic data\n",
    "synthetic_data = copula.sample(10)  # Adjust sample size as needed\n",
    "\n",
    "\n",
    "# Save synthetic data\n",
    "synthetic_data.to_csv('synthetic_data2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator()\n",
    "z = Input(shape=(1,))\n",
    "img = generator(z)\n",
    "validity = discriminator(img)\n",
    "combined = Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn.fit(new_X.ravel().reshape(-1, 1), new_y.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim + 1),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(2,)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "optimizer_gen = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "optimizer_disc = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(optimizer=optimizer_disc, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator(5)  # Latent dim\n",
    "\n",
    "# Combine models\n",
    "z = layers.Input(shape=(5,))\n",
    "x_input = layers.Input(shape=(1,))\n",
    "gen_y = generator(layers.concatenate([z, x_input]))  # Generator now also takes x_input\n",
    "combined_input = layers.Concatenate()([x_input, gen_y])\n",
    "valid = discriminator(combined_input)\n",
    "combined = tf.keras.Model([x_input, z], valid)\n",
    "combined.compile(optimizer=optimizer_gen, loss='binary_crossentropy')\n",
    "\n",
    "def train(epochs, batch_size=128):\n",
    "    x_data = np.linspace(-10, 10, 1000)\n",
    "    y_data = 2 * x_data + 3 + np.random.normal(0, 2, x_data.shape)\n",
    "    x_data = (x_data - x_data.min()) / (x_data.max() - x_data.min())\n",
    "\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, x_data.shape[0], batch_size)\n",
    "        x_batch = x_data[idx].reshape(-1, 1)\n",
    "        y_batch = y_data[idx].reshape(-1, 1)\n",
    "        noise = np.random.normal(0, 1, (batch_size, 5))\n",
    "        \n",
    "        gen_y_batch = generator.predict(np.hstack((noise, x_batch)))  # Provide x values as input\n",
    "        d_loss_real = discriminator.train_on_batch(np.hstack((x_batch, y_batch)), valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(np.hstack((x_batch, gen_y_batch)), fake)\n",
    "        \n",
    "        g_loss = combined.train_on_batch([x_batch, noise], valid)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, D loss: {0.5 * np.add(d_loss_real, d_loss_fake)[0]}, G loss: {g_loss}\")\n",
    "\n",
    "    plt.scatter(x_data, y_data, color='blue', label='Original Data')\n",
    "    plt.scatter(x_data, generator.predict(np.hstack((np.random.normal(0, 1, (1000, 5)), x_data.reshape(-1, 1)))), color='red', label='Generated Data')\n",
    "    plt.title(\"Comparison of Original and Generated Data\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "train(epochs=300, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim + 1),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(2,)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "optimizer_gen = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "optimizer_disc = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(optimizer=optimizer_disc, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator(5)  # Latent dim\n",
    "\n",
    "# Combine models\n",
    "z = layers.Input(shape=(5,))\n",
    "x_input = layers.Input(shape=(1,))\n",
    "gen_input = layers.Concatenate()([z, x_input])\n",
    "gen_y = generator(gen_input)  # Generator now also takes x_input\n",
    "combined_input = layers.Concatenate()([x_input, gen_y])\n",
    "valid = discriminator(combined_input)\n",
    "combined = tf.keras.Model([x_input, z], valid)\n",
    "combined.compile(optimizer=optimizer_gen, loss='binary_crossentropy')\n",
    "\n",
    "def train(epochs, batch_size=128):\n",
    "    x_data = np.linspace(-10, 10, 1000)\n",
    "    y_data = 2 * x_data + 3 + np.random.normal(0, 2, x_data.shape)\n",
    "    x_data = (x_data - x_data.min()) / (x_data.max() - x_data.min())\n",
    "\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, x_data.shape[0], batch_size)\n",
    "        x_batch = x_data[idx].reshape(-1, 1)\n",
    "        y_batch = y_data[idx].reshape(-1, 1)\n",
    "        noise = np.random.normal(0, 1, (batch_size, 5))\n",
    "        \n",
    "        gen_y_batch = generator.predict(np.hstack((noise, x_batch)))  # Provide x values as input\n",
    "        d_loss_real = discriminator.train_on_batch(np.hstack((x_batch, y_batch)), valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(np.hstack((x_batch, gen_y_batch)), fake)\n",
    "        \n",
    "        g_loss = combined.train_on_batch([x_batch, noise], valid)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, D loss: {0.5 * np.add(d_loss_real, d_loss_fake)[0]}, G loss: {g_loss}\")\n",
    "\n",
    "    plt.scatter(x_data, y_data, color='blue', label='Original Data')\n",
    "    x_points = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "    noise = np.random.normal(0, 1, (1000, 5))\n",
    "    generated_y = generator.predict(np.hstack((noise, x_points)))\n",
    "    plt.scatter(x_points, generated_y, color='red', label='Generated Data')\n",
    "    plt.title(\"Comparison of Original and Generated Data\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "train(epochs=300, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=latent_dim + 1),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(2,)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "optimizer_gen = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "optimizer_disc = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(optimizer=optimizer_disc, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator(5)  # Latent dim\n",
    "\n",
    "# Combine models\n",
    "z = layers.Input(shape=(5,))\n",
    "x_input = layers.Input(shape=(1,))\n",
    "gen_input = layers.Concatenate()([z, x_input])\n",
    "gen_y = generator(gen_input)  # Generator now also takes x_input\n",
    "combined_input = layers.Concatenate()([x_input, gen_y])\n",
    "valid = discriminator(combined_input)\n",
    "combined = tf.keras.Model([x_input, z], valid)\n",
    "combined.compile(optimizer=optimizer_gen, loss='binary_crossentropy')\n",
    "\n",
    "def train(epochs, batch_size=128):\n",
    "    x_data = np.linspace(-10, 10, 1000)\n",
    "    y_data = 2 * x_data + 3 + np.random.normal(0, 2, x_data.shape)\n",
    "    x_data = (x_data - x_data.min()) / (x_data.max() - x_data.min())\n",
    "\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, x_data.shape[0], batch_size)\n",
    "        x_batch = x_data[idx].reshape(-1, 1)\n",
    "        y_batch = y_data[idx].reshape(-1, 1)\n",
    "        noise = np.random.normal(0, 1, (batch_size, 5))\n",
    "        \n",
    "        gen_y_batch = generator.predict(np.hstack((noise, x_batch)))  # Provide x values as input\n",
    "        d_loss_real = discriminator.train_on_batch(np.hstack((x_batch, y_batch)), valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(np.hstack((x_batch, gen_y_batch)), fake)\n",
    "        \n",
    "        g_loss = combined.train_on_batch([x_batch, noise], valid)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, D loss: {0.5 * np.add(d_loss_real, d_loss_fake)[0]}, G loss: {g_loss}\")\n",
    "\n",
    "    plt.scatter(x_data, y_data, color='blue', label='Original Data')\n",
    "    x_points = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "    noise = np.random.normal(0, 1, (1000, 5))\n",
    "    generated_y = generator.predict(np.hstack((noise, x_points)))\n",
    "    plt.scatter(x_points, generated_y, color='red', label='Generated Data')\n",
    "    plt.title(\"Comparison of Original and Generated Data\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "train(epochs=200, batch_size=16)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
